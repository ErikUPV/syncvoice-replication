pretrained_path: base_model
train_manifest: data/train.jsonl
val_manifest: data/valid.jsonl
sample_rate: 44100
batch_size: 16
grad_accum_steps: 1  # Gradient accumulation steps, >1 can increase effective batch size without increasing memory
num_workers: 4
num_iters: 20000
log_interval: 20
valid_interval: 500
save_interval: 4000
learning_rate: 0.00002 
weight_decay: 0.01
warmup_steps: 1000
max_steps: 20000
max_batch_tokens: 8192  # Example: single batch can have at most 16k tokens, with batch_size=4, each sample can have at most 4096 tokens
save_path: models/syncvoice_voxcpm_0.00002
tensorboard: logs/syncvoice_voxcpm_0.00002
lambdas:
  loss/diff: 1.0
  loss/stop: 1.0
